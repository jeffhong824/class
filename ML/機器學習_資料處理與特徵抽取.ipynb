{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aedda387-c7f5-42cf-9214-a39d6191f423",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ML "
   ]
  },
  {
   "cell_type": "raw",
   "id": "079a5ac9-c1af-442a-8b52-d8849ab62539",
   "metadata": {},
   "source": [
    "tot(場站總停車格)\n",
    "sbi(場站目前車輛數量)\n",
    "bemp(空位數量)\n",
    "act(全站禁用狀態)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4543d4ac-4655-43a8-9224-50fc405d892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import datasets, models, layers, utils, activations, losses, optimizers, metrics # DNN 系列\n",
    "from tensorflow.keras.layers import Dense, LSTM,Bidirectional,SimpleRNN,GRU,Activation # RNN系列\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,precision_score,recall_score,f1_score\n",
    "import datetime\n",
    "from IPython.display import clear_output\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938eb709-2118-436e-adaa-319131d02544",
   "metadata": {},
   "source": [
    "#### 收集&整理數據路徑\n",
    "##### 期望得到: \n",
    "* target_sno: 提交csv所需sno\n",
    "* date_dictionary: 包含train~test範圍的 data，以及對應有沒有得到資料\n",
    "* sno_dictionary: 包含sno與其資料的對應路徑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff5b7d46-9a1d-4b38-9b17-024107409cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 確認主要想預測的 sno\n",
    "def collect_target_sno(_path):\n",
    "    '''\n",
    "    Readme\n",
    "    \n",
    "    > input:\n",
    "        _path # 提交格式csv檔案路徑\n",
    "    \n",
    "    > output\n",
    "        target_sno # 提交csv所需sno\n",
    "    \n",
    "    '''\n",
    "    sample_csv = pd.read_csv(_path)\n",
    "    target_sno = {each.split('_')[1] for each in sample_csv['id']}\n",
    "    print('finish target_sno: ', len(target_sno)) # 期望數量\n",
    "    return list(target_sno)\n",
    "\n",
    "## 收集&整理數據\n",
    "def collect_organize_data(release_path, date_train):\n",
    "    '''\n",
    "    Readme\n",
    "    \n",
    "    input:\n",
    "    release_path # data主資料夾路徑\n",
    "    date_train # 期望會有data的日期\n",
    "    \n",
    "    output:\n",
    "    date_dictionary # 包含train~test範圍的 data，以及對應有沒有得到資料\n",
    "    sno_dictionary # 包含sno與其資料的對應路徑\n",
    "    '''\n",
    "    \n",
    "    # phase1 : 遍歷資料夾，以list方式儲存資訊，資訊包含 \"有data的日期\"、\"資料包含的車站\"、\"各車站下所有檔案路徑\"\n",
    "    date_get_data = [] # 實際上有data的日期\n",
    "    file_paths_station, file_paths_station_date = [], [] # 資料包含的車站, 各車站下所有檔案路徑\n",
    "    for root, dirs, files in os.walk(release_path): # 遍歷資料夾\n",
    "        if root != release_path: # 是具有日期的path\n",
    "            date = root.replace(release_path,'') # 保留日期 \n",
    "            if date in date_train: # 檢查是否存在\n",
    "                date_get_data.append(date) # 儲存日期\n",
    "        for file in files: # 每個子資料夾下的檔案\n",
    "            if file.endswith('.json'): # 是\".json\"檔\n",
    "                sno = file.replace('.json','') # 確認sno號(車站號)\n",
    "                file_path = os.path.join(root, file)\n",
    "                if sno not in file_paths_station: # 如果是新的車站\n",
    "                    file_paths_station.append(sno) # 加入車站號\n",
    "                    file_paths_station_date.append([file_path])\n",
    "                else:\n",
    "                    station_position = file_paths_station.index(sno)\n",
    "                    file_paths_station_date[station_position].append(file_path)\n",
    "    # phase2-1: 檔案整理_date\n",
    "    date_dictionary = {}\n",
    "    for each_date in date_train:\n",
    "        get_data = each_date in date_get_data # 有 get到date則True否則False\n",
    "        date_dictionary[each_date] = get_data\n",
    "    \n",
    "    # phase2-2: 檔案整理_sno\n",
    "    sno_dictionary = {}\n",
    "    for sno_order, sno in enumerate(file_paths_station):\n",
    "        sno_dictionary[sno] = file_paths_station_date[sno_order]\n",
    "    \n",
    "    print('Complete collection & organization of data', flush=True)\n",
    "    return  date_dictionary,  sno_dictionary   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "631c5621-fe3d-4332-844a-c354662f93c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1316.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5264 / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21542e2b-f466-404d-b186-0c33f4c0aace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish target_sno:  112\n",
      "Complete collection & organization of data\n"
     ]
    }
   ],
   "source": [
    "# input path\n",
    "project_path = 'D:/在職進修/修課/機器學習/Final_project/'\n",
    "release_path = project_path+'html.2023.final.data-release/release/'\n",
    "sample_csv_list = ['sample_submission_stage'+str(eg) for eg in range(1,4)]\n",
    "\n",
    "# collect_target_sno\n",
    "target_sno = collect_target_sno(project_path+sample_csv_list[0]+'.csv')\n",
    "\n",
    "# collect_organize_data\n",
    "date_ranges = [range(20231002, 20231032), range(20231101, 20231131), range(20231201, 20231226)]\n",
    "date_train = [str(date_int) for r in date_ranges for date_int in r]\n",
    "date_dictionary, sno_dictionary = collect_organize_data(release_path, date_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd34f330-6782-40a5-993d-0bc0e01a7531",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 讀取 & 清理數據-缺失值處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2286a682-5b40-423d-9856-689de620158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_negative_ones(df, columns):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].replace(-1, np.nan).fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "def day_of_week(year_month_day):\n",
    "    year, month, day = int(year_month_day[:4]), int(year_month_day[4:6]), int(year_month_day[6:])\n",
    "    return datetime.date(year, month, day).strftime(\"%A\")\n",
    "\n",
    "def time_feature(date, timestamp_each):\n",
    "    hour, minute = map(int, timestamp_each.split(':'))\n",
    "    minuate_accumulation = hour * 60 + minute\n",
    "    day = day_of_week(date)\n",
    "    day_code = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"].index(day) + 1\n",
    "    holiday = -1 if day_code <= 5 else 1\n",
    "    if date in [\"20231010\",\"20231225\"]:\n",
    "        holiday = 1 \n",
    "    return hour, minute, minuate_accumulation, day_code, holiday\n",
    "\n",
    "def process_file(sno_name, file_list, save_path, station_line=500101001):\n",
    "    date_list, sno_name_list, hour_list, minute_list, minute_accumulation_list, day_code_list, holiday_list, tot_list, act_list, sbi_list, index_time = [], [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "    for file_path in file_list:\n",
    "        date = file_path.split('/')[-1].split('\\\\')[0]\n",
    "        sno = file_path.split('\\\\')[1].split('.')[0]\n",
    "\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        pre_tot = pre_act = pre_sbi = -1\n",
    "        for timestamp_each in data.keys():\n",
    "            hour, minute, minute_accumulation, day_code, holiday = time_feature(str(date), timestamp_each)\n",
    "\n",
    "            date_list.append(date)\n",
    "            sno_name_list.append(int(sno) / station_line)\n",
    "            hour_list.append(hour)\n",
    "            minute_list.append(minute)\n",
    "            minute_accumulation_list.append(minute_accumulation)\n",
    "            day_code_list.append(day_code)\n",
    "            holiday_list.append(holiday)\n",
    "            index_time.append(f\"Time:{date}{minute_accumulation}\")\n",
    "\n",
    "            tot = data[timestamp_each].get('tot', pre_tot if pre_tot != -1 else -1)\n",
    "            act = data[timestamp_each].get('act', pre_act if pre_act != -1 else -1)\n",
    "            sbi = data[timestamp_each].get('sbi', pre_sbi if pre_sbi != -1 else -1)\n",
    "\n",
    "            pre_tot, pre_act, pre_sbi = tot, act, sbi\n",
    "            tot_list.append(tot)\n",
    "            act_list.append(act)\n",
    "            sbi_list.append(sbi)\n",
    "\n",
    "    train_feature1 = pd.DataFrame({\n",
    "        'date': date_list, 'sno': sno_name_list, 'hour': hour_list, 'minute': minute_list,\n",
    "        'minute_accumulation': minute_accumulation_list, 'day_code': day_code_list, 'holiday': holiday_list\n",
    "    })\n",
    "\n",
    "    train_feature2 = pd.DataFrame({'tot_acc': tot_list, 'act_acc': act_list, 'sbi_acc': sbi_list})\n",
    "    replace_negative_ones(train_feature2, ['tot_acc', 'act_acc', 'sbi_acc'])\n",
    "\n",
    "    train_feature = pd.concat([train_feature1, train_feature2], axis=1)\n",
    "    train_feature.index = index_time\n",
    "    train_feature.to_json(os.path.join(save_path, f'{sno_name}.json'))\n",
    "\n",
    "def process_raw_data_into_minute_by_minute_features(sno_dictionary, save_path, station_line=500101001):\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        for sno_name, file_list in sno_dictionary.items():\n",
    "            if not os.path.exists(os.path.join(save_path, f'{sno_name}.json')):\n",
    "                executor.submit(process_file, sno_name, file_list, save_path, station_line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "484dbd09-f78f-47f7-92b6-a4a79f926a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_phase1 = project_path+'data_science_phase1_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fb34ab3-2e02-4406-ba76-a14af7bf6bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_raw_data_into_minute_by_minute_features(sno_dictionary, save_path_phase1, station_line=500101001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e65407b-f3b3-487a-b65b-7a6d7b358c44",
   "metadata": {},
   "source": [
    "#### 特徵工程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b55fa2-9c09-48c1-bc9b-85ca441f877f",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### feature1_day\n",
    "* feature\n",
    "    * Station characteristics * 1\n",
    "    * Station upper bound * 1\n",
    "    * date characteristics * 2 (星期、放假)\n",
    "    * time characteristics * 3 (小時、分鐘、累積)\n",
    "    * 1 day time step 72 * 1\n",
    "* --> input 72 * 8\n",
    "* --> pred 72 * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df4d57b7-2a22-439e-96e9-371d1bf78477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1440"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "24*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fd5987fd-ff48-4970-8202-cd9ebde5c52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_phase2 = project_path+'data_science_phase2_feature/feature1_day/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "592ab9e6-f4b5-4781-a9c3-110d55c1d648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'500106024'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d4e99ce0-5281-4dea-880e-5a74e8a04e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███████████████████████████▏                                                   | 453/1317 [00:36<00:32, 26.47it/s]C:\\Users\\Tingchun.TC.Hung\\Anaconda3\\envs\\NLP\\lib\\site-packages\\ipykernel_launcher.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "C:\\Users\\Tingchun.TC.Hung\\Anaconda3\\envs\\NLP\\lib\\site-packages\\ipykernel_launcher.py:84: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1317/1317 [1:44:43<00:00,  4.77s/it]\n"
     ]
    }
   ],
   "source": [
    "# 讀出每一筆 phase1 data\n",
    "X_train_all = []\n",
    "y_train_all = []\n",
    "X_test_all = []\n",
    "y_test_all = []\n",
    "X_test_all_target_sno = []\n",
    "y_test_all_target_sno = []\n",
    "non_all_date_sno = []\n",
    "for sno in tqdm(list(sno_dictionary.keys())[:-10]): \n",
    "    load_path = save_path_phase1 + sno + '.json'\n",
    "    save_path = save_path_phase2 + sno + '.json'\n",
    "    try:\n",
    "        X_train = np.load(save_path_phase2+sno+'_X_train.npy') \n",
    "        y_train = np.load(save_path_phase2+sno+'_y_train.npy') \n",
    "        X_test = np.load(save_path_phase2+sno+'_X_test.npy') \n",
    "        y_test = np.load(save_path_phase2+sno+'_y_test.npy') \n",
    "        X_train_all.extend(X_train)\n",
    "        y_train_all.extend(y_train)\n",
    "        X_test_all.extend(X_test)\n",
    "        y_test_all.extend(y_test)\n",
    "        if sno in target_sno: # 是想關注的sno 才做為 testing set\n",
    "            X_test_all_target_sno.extend(X_test)\n",
    "            y_test_all_target_sno.extend(y_test)\n",
    "        need_run = False\n",
    "    except:\n",
    "        need_run = True\n",
    "\n",
    "    if need_run:\n",
    "        with open(load_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        sorted_features_df = pd.DataFrame(data)\n",
    "        \n",
    "        # classify data-feature\n",
    "        def create_date_pairs(date_dict, date_range):\n",
    "            date_pairs = {}\n",
    "            dates = list(date_dict.keys())[date_range]\n",
    "            for i in range(len(dates) - 1):\n",
    "                if date_dict[dates[i]] and date_dict[dates[i + 1]]:\n",
    "                    date_pairs[dates[i]] = dates[i + 1]\n",
    "            return date_pairs\n",
    "\n",
    "        train_date = create_date_pairs(date_dictionary, slice(None, -8))\n",
    "        test_date = create_date_pairs(date_dictionary, slice(-9, -1))\n",
    "\n",
    "        X_train_date = [sorted_features_df[sorted_features_df['date'] == train_date_each] for train_date_each in train_date.keys()]\n",
    "        y_train_date = [sorted_features_df[sorted_features_df['date'] == train_date[train_date_each]] for train_date_each in train_date.keys()]\n",
    "\n",
    "        X_test_date = [sorted_features_df[sorted_features_df['date'] == test_date_each] for test_date_each in test_date.keys()]\n",
    "        y_test_date = [sorted_features_df[sorted_features_df['date'] == test_date[test_date_each]] for test_date_each in test_date.keys()]\n",
    "\n",
    "        def create_date_sets(X_date, step=20):\n",
    "            date_sets_all = []\n",
    "            for X_date_each in X_date:\n",
    "                now_focus_df = X_date_each.copy()\n",
    "                # now_focus_df['normalize_sno'] = now_focus_df['sno']\n",
    "                # now_focus_df['normalize_tot_acc'] = now_focus_df['tot_acc']\n",
    "                now_focus_df['normalize_day_code'] = now_focus_df['day_code'] / 7\n",
    "                # now_focus_df['normalize_holiday'] = now_focus_df['holiday']\n",
    "                now_focus_df['normalize_hour'] = now_focus_df['hour'] / 23\n",
    "                now_focus_df['normalize_minute'] = now_focus_df['minute'] / 59\n",
    "                now_focus_df['normalize_minute_accumulation'] = now_focus_df['minute_accumulation'] / 1439\n",
    "                now_focus_df['normalize_sbi'] = now_focus_df['sbi_acc'] / now_focus_df['tot_acc']\n",
    "\n",
    "                date_sets = []\n",
    "                for each_start in range(0, step):\n",
    "                    loc_list = now_focus_df.index[each_start::step].tolist()\n",
    "                    date_set_one = now_focus_df.loc[loc_list]\n",
    "                    date_sets.append(date_set_one)\n",
    "                date_sets_all.append(date_sets)\n",
    "            return date_sets_all\n",
    "\n",
    "        date_X_training_set = create_date_sets(X_train_date)\n",
    "        date_y_training_set = create_date_sets(y_train_date)\n",
    "        date_X_testing_set = create_date_sets(X_test_date)\n",
    "        date_y_testing_set = create_date_sets(y_test_date)\n",
    "\n",
    "        # 提取訓練數據\n",
    "        X_train = np.array([[df['sno'].values, df['tot_acc'].values, df['normalize_day_code'].values, df['holiday'].values, df['normalize_hour'].values, df['normalize_minute'].values, df['normalize_minute_accumulation'].values, df['normalize_sbi'].values] for dataset in date_X_training_set for df in dataset])\n",
    "        y_train = np.array([[df['sno'].values, df['tot_acc'].values, df['normalize_day_code'].values, df['holiday'].values, df['normalize_hour'].values, df['normalize_minute'].values, df['normalize_minute_accumulation'].values, df['normalize_sbi'].values] for dataset in date_y_training_set for df in dataset])\n",
    "\n",
    "        # 提取測試數據\n",
    "        X_test = np.array([[df['sno'].values, df['tot_acc'].values, df['normalize_day_code'].values, df['holiday'].values, df['normalize_hour'].values, df['normalize_minute'].values, df['normalize_minute_accumulation'].values, df['normalize_sbi'].values] for dataset in date_X_testing_set for df in dataset])\n",
    "        y_test = np.array([[df['sno'].values, df['tot_acc'].values, df['normalize_day_code'].values, df['holiday'].values, df['normalize_hour'].values, df['normalize_minute'].values, df['normalize_minute_accumulation'].values, df['normalize_sbi'].values] for dataset in date_y_testing_set for df in dataset])\n",
    "        \n",
    "        try:\n",
    "            X_train = np.transpose(X_train, (0, 2, 1))\n",
    "            y_train = np.transpose(y_train, (0, 2, 1))\n",
    "            X_test = np.transpose(X_test, (0, 2, 1))\n",
    "            y_test = np.transpose(y_test, (0, 2, 1))\n",
    "            size_correct = True\n",
    "        except:\n",
    "            size_correct = False\n",
    "            non_all_date_sno.append(sno)\n",
    "        if size_correct:\n",
    "            np.save(save_path_phase2+sno+'_X_train.npy', X_train)\n",
    "            np.save(save_path_phase2+sno+'_y_train.npy', y_train)\n",
    "            np.save(save_path_phase2+sno+'_X_test.npy', X_test)\n",
    "            np.save(save_path_phase2+sno+'_y_test.npy', y_test)\n",
    "        \n",
    "            X_train_all.extend(X_train)\n",
    "            y_train_all.extend(y_train)\n",
    "            X_test_all.extend(X_test)\n",
    "            y_test_all.extend(y_test)\n",
    "            if sno in target_sno: # 是想關注的sno 才做為 testing set\n",
    "                X_test_all_target_sno.extend(X_test)\n",
    "                y_test_all_target_sno.extend(y_test)\n",
    "        \n",
    "np.save(save_path_phase2+'X_train.npy', np.array(X_train_all))\n",
    "np.save(save_path_phase2+'y_train.npy', np.array(y_train_all))\n",
    "np.save(save_path_phase2+'X_test.npy', np.array(X_test_all))\n",
    "np.save(save_path_phase2+'y_test.npy', np.array(y_test_all)) \n",
    "np.save(save_path_phase2+'X_test_target_sno.npy', np.array(X_test_all_target_sno))\n",
    "np.save(save_path_phase2+'y_test_target_sno.npy', np.array(y_test_all_target_sno)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8d9eab34-19d0-4a0c-905d-8b9e737ad8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(save_path_phase2+'_X_train.npy') \n",
    "y_train = np.load(save_path_phase2+'_y_train.npy') \n",
    "X_test = np.load(save_path_phase2+'_X_test_target_sno.npy') \n",
    "y_test = np.load(save_path_phase2+'_y_test_target_sno.npy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "310ce807-066b-473c-8377-dc5a92ac96ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1764780, 72), (1764780, 72), (15680, 72), (15680, 72))"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
